{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee2f501",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis with LSTM**\n",
    "\n",
    "This jupyter notebook performs sentiment analysis on the Rotten Tomatoes dataset using a **Long Short-Term Memory (LSTM)** neural network implemented in PyTorch. The dataset consists of phrases labeled with sentiment scores (0-4). Below is an overview of the workflow, model architecture, and results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e47856",
   "metadata": {},
   "source": [
    "## **Workflow**\n",
    "\n",
    "### **1. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74392ea190ca49ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#Check GPU Availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26bda69",
   "metadata": {},
   "source": [
    "### **2. Prepare Data**\n",
    "- Convert TF-IDF features to PyTorch tensors.\n",
    "- Create a custom dataset class for handling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66139e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TF-IDF data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_vec2, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_val_vec2, dtype=torch.float32).to(device)\n",
    "\n",
    "# Custom dataset class\n",
    "class PhraseDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = features  # Features\n",
    "        self.labels = labels  # Labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)  # Dataset length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]  # Return features and labels\n",
    "        else:\n",
    "            return self.features[idx]  # Return only features\n",
    "\n",
    "# Prepare dataset\n",
    "train_labels = train['Sentiment'].values\n",
    "train_dataset = PhraseDataset(X_train_tensor, train_labels)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e2da4",
   "metadata": {},
   "source": [
    "## **Model Architecture**\n",
    "\n",
    "### **3. Define LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de82051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(64, 32)  # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(32, output_size)  # Output layer\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer to prevent overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = F.relu(self.fc2(x))  # Apply ReLU activation\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc93028",
   "metadata": {},
   "source": [
    "### **4. Initialize Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c763b1bb",
   "metadata": {},
   "source": [
    "input_dim = X_train.shape[1]  # Input dimension\n",
    "output_size = 5  # Number of output classes\n",
    "net = SentimentNN(input_dim, output_size).to(device)  # Move model to device\n",
    "net.train()  # Set model to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a861a9d",
   "metadata": {},
   "source": [
    "### **5. Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587312fb985f34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "epochs = 100  # Number of epochs\n",
    "lr = 0.001  # Learning rate\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)  # Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "\n",
    "# Early Stopping\n",
    "best_val_acc = 0  # Best validation accuracy\n",
    "patience = 10  # Patience for early stopping\n",
    "counter = 0  # Early stopping counter\n",
    "\n",
    "# Training Loop\n",
    "for e in range(epochs):\n",
    "    net.train()  # Set model to training mode\n",
    "    running_loss = 0.0  # Track training loss\n",
    "    running_acc = 0.0  # Track training accuracy\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        output = net(inputs)  # Forward pass\n",
    "        loss = criterion(output, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "        running_acc += (output.argmax(dim=1) == labels).float().mean()  # Accumulate accuracy\n",
    "\n",
    "    print(f\"Epoch {e + 1}/{epochs}, Loss: {running_loss / len(train_loader):.6f}, Acc: {running_acc / len(train_loader):.6f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    net.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0  # Track validation loss\n",
    "    val_acc = 0.0  # Track validation accuracy\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_output = net(val_inputs)  # Forward pass\n",
    "            val_loss += criterion(val_output, val_labels).item()  # Accumulate loss\n",
    "            val_acc += (val_output.argmax(dim=1) == val_labels).float().mean().item()  # Accumulate accuracy\n",
    "\n",
    "    val_acc /= len(val_loader)  # Compute average validation accuracy\n",
    "    print(f\"Validation Loss: {val_loss / len(val_loader):.6f}, Validation Accuracy: {val_acc:.6f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc  # Update best validation accuracy\n",
    "        counter = 0  # Reset early stopping counter\n",
    "    else:\n",
    "        counter += 1  # Increment early stopping counter\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")  # Trigger early stopping\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bffa41",
   "metadata": {},
   "source": [
    "### **6. Testing and Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()  # Set model to evaluation mode\n",
    "test_predictions = []  # Store test predictions\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    test_loader = DataLoader(PhraseDataset(X_test_tensor), batch_size=32)  # Create test data loader\n",
    "    for test_inputs in test_loader:\n",
    "        test_inputs = test_inputs.to(device)  # Move data to device\n",
    "        test_output = net(test_inputs)  # Forward pass\n",
    "        test_predictions.extend(test_output.argmax(dim=1).cpu().numpy())  # Store predictions\n",
    "\n",
    "# Create output DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'PhraseId': test['PhraseId'],  # Ensure 'PhraseId' matches the test set\n",
    "    'Sentiment': test_predictions\n",
    "})\n",
    "\n",
    "# Save predictions to CSV\n",
    "output_path = 'E:/shuju/answer/predictions.csv'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)  # Create output directory if it doesn't exist\n",
    "output_df.to_csv(output_path, index=False)  # Save predictions\n",
    "print(f\"Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66532959",
   "metadata": {},
   "source": [
    "## **Results**\n",
    "- **Test Predictions**: Saved to `predictions.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
